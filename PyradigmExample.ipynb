{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "* <a href='#motivation'>Motivation</a>\n",
    "\n",
    "* <a href='#constructor'>Constructor</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Serialization</a>\n",
    "\n",
    "* <a href='#arithmetic'>Arithmetic</a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python data structure to improve handling of datasets in machine learning workflows\n",
    "\n",
    "This class is greatly suited for neuroimaging applications (or any other domain), where each sample needs to be uniquely identified with a subject ID (or something similar). \n",
    "\n",
    "Key-level correspondence across data, labels (1 or 2), classnames ('healthy', 'disease') and the related helps maintain data integrity and improve the provenance, in addition to enabling traceback to original sources from where the features have been originally derived.\n",
    "\n",
    "Just to given you a concrete examples, let's look at how an ML dataset is handled traditionally.\n",
    "\n",
    "You have a matrix X of size n x p, with n samples and p features, and a vector y containing the target values (or class labels or class identifiers). This X and y serves as training (and test set) for a classifier like SVM to fit the data X to match y as accurately as possible.\n",
    "\n",
    "Let's get a little more concrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "X : \n",
      "[[ 0.17  0.62  0.8 ]\n",
      " [ 0.65  0.83  0.55]\n",
      " [ 0.59  0.03  0.09]\n",
      " [ 0.76  0.27  0.28]\n",
      " [ 0.81  0.26  0.68]\n",
      " [ 0.77  0.58  0.03]\n",
      " [ 0.83  0.2   0.88]\n",
      " [ 0.75  0.07  0.43]\n",
      " [ 0.53  0.51  0.88]\n",
      " [ 0.52  0.48  0.7 ]]\n",
      "y : \n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10 # number of samples\n",
    "p = 3  # number of features\n",
    "\n",
    "X = np.random.random([n, p]) # random data for illustration\n",
    "y = [1]*5 + [2]*5            # random labels ...\n",
    "\n",
    "np.set_printoptions(precision=2) # save some screen space\n",
    "print 'X : \\n', X\n",
    "print 'y : \\n', y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the machine learning toolboxes take their input in this form: X and y, regardless of the original source that produced these features in the first place.\n",
    "\n",
    "This is all fine if all you ever wanted to do is to extract some features, do some machine learning and dispose these features away! \n",
    "\n",
    "** But this is almost never the case!**\n",
    "\n",
    "Because it doesn't simply end there.\n",
    "\n",
    "At a minimum, I often need to know \n",
    " * which samples are misclassified - meaning you need to know what the identifiers are and not simply their row indices in X?\n",
    " * what are the charecteristics of those samples?\n",
    " * what classes do they belong to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all this info needs to be obtained\n",
    " * without having to write lots of code connecting few non-obvious links to disparate sources of data (numerical features X, and sample identifiers in a CSV file) to find the relevant info\n",
    " * without having to track down who or which method originally produced these features\n",
    " * how the previous personnel or grad student organized the whole dataset, if you haven't generated the features yourself from scratch\n",
    "\n",
    "And if you are like me, you would be thinking about how would you organize your workflow such that the aforementioned tasks can be accomplished with ease.\n",
    " \n",
    "This data structure attempts to accomplish that with ease. By always organizing the extracted features keyed-in into a dictionary with their *sample id*, and other important info such as *target values* and other identified info. This, by definition, preserves the integrity of the data (inability to incorrectly label samples etc).\n",
    "\n",
    "No, this data structure doesn't offer the full [provenance tracking](http://rrcns.readthedocs.io/en/latest/provenance_tracking.html), which is quite a challenging problem. But it tries make your life a little easier in your ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example application is shown below, touching upon the following topics:\n",
    "\n",
    "* <a href='#constructor'>Constructor</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Serialization</a>\n",
    "\n",
    "* <a href='#arithmetic'>Arithmetic</a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improting the necessary modules and our fancy class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyradigm import MLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate it and give it a description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = MLDataset()\n",
    "dataset.description = 'ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "Empty dataset."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the dataset some description attached to it, however we know it is empty. This can be verified in a boolean context as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's add samples to this dataset which is when this dataset implementation becomes really handy. Before we do that, we will define some convenience routines defined to just illustrate a simple yet common use of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_thickness(path):\n",
    "    \"\"\"Dummy function to minic a data reader.\"\"\"\n",
    "\n",
    "    # in your actural routine, this might be:\n",
    "    #   pysurfer.read_thickness(path).values()\n",
    "    return np.random.random(2)\n",
    "\n",
    "\n",
    "def get_features(work_dir, subj_id):\n",
    "    \"\"\"Returns the whole brain cortical thickness for a given subject ID.\"\"\"\n",
    "\n",
    "    # extension to identify the data file; this could be .curv, anything else you choose\n",
    "    ext_thickness = '.thickness'\n",
    "\n",
    "    thickness = dict()\n",
    "    for hemi in ['lh', 'rh']:\n",
    "        path_thickness = os.path.join(work_dir, subj_id, hemi + ext_thickness)\n",
    "        thickness[hemi] = read_thickness(path_thickness)\n",
    "\n",
    "    # concatenating them to build a whole brain feature set\n",
    "    thickness_wb = np.concatenate([thickness['lh'], thickness['rh']])\n",
    "\n",
    "    return thickness_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have IO routines to read the data for us. Let's define where the data will come from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_dir = '/project/ADNI/FreesurferThickness_v4p3'\n",
    "class_set = ['Cntrl', 'Alzmr', 'MCI']\n",
    "class_sizes = [15, 12, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would obviously change for your applications, but this has sufficient properties to illustrate the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what methods this dataset offers us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_classes',\n",
       " 'add_sample',\n",
       " 'class_set',\n",
       " 'class_sizes',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'data_matrix',\n",
       " 'del_sample',\n",
       " 'description',\n",
       " 'extend',\n",
       " 'get_class',\n",
       " 'get_feature_subset',\n",
       " 'get_subset',\n",
       " 'glance',\n",
       " 'keys',\n",
       " 'num_classes',\n",
       " 'num_features',\n",
       " 'num_samples',\n",
       " 'random_subset',\n",
       " 'sample_ids',\n",
       " 'save',\n",
       " 'target']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='constructor'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor\n",
    "\n",
    "You can see there few methods such as add_sample, get_subset etc: important method being add_sample, which is key to constructing this dataset. Let's go ahead and some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "\n",
    "def read_target_list(class_name, class_size):\n",
    "    \"Generates a random target list. In reality, you would do something like the commented code below.\"\n",
    "    target_list = list()\n",
    "    for idx in range(class_size):\n",
    "        target_list.append('{}{:04d}'.format(class_name[0],np.random.randint(1000)))\n",
    "        \n",
    "    return target_list\n",
    "\n",
    "#     target_list_path = os.path.join(work_dir,'scripts','test_sample.{}'.format(class_name))\n",
    "#     with open(target_list_path,'r') as tf:\n",
    "#         target_list = tf.readlines()\n",
    "#         target_list = [sub.strip() for sub in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on class Cntrl\n",
      "\t reading subject           C0521\n",
      "\t reading subject           C0097\n",
      "\t reading subject           C0195\n",
      "\t reading subject           C0846\n",
      "\t reading subject           C0072\n",
      "\t reading subject           C0074\n",
      "\t reading subject           C0299\n",
      "\t reading subject           C0232\n",
      "\t reading subject           C0847\n",
      "\t reading subject           C0427\n",
      "\t reading subject           C0380\n",
      "\t reading subject           C0888\n",
      "\t reading subject           C0078\n",
      "\t reading subject           C0164\n",
      "\t reading subject           C0662\n",
      "Working on class Alzmr\n",
      "\t reading subject           A0048\n",
      "\t reading subject           A0060\n",
      "\t reading subject           A0914\n",
      "\t reading subject           A0304\n",
      "\t reading subject           A0339\n",
      "\t reading subject           A0363\n",
      "\t reading subject           A0132\n",
      "\t reading subject           A0645\n",
      "\t reading subject           A0547\n",
      "\t reading subject           A0943\n",
      "\t reading subject           A0266\n",
      "\t reading subject           A0207\n",
      "Working on class   MCI\n",
      "\t reading subject           M0804\n",
      "\t reading subject           M0084\n",
      "\t reading subject           M0945\n",
      "\t reading subject           M0518\n",
      "\t reading subject           M0940\n",
      "\t reading subject           M0365\n",
      "\t reading subject           M0222\n",
      "\t reading subject           M0441\n",
      "\t reading subject           M0725\n",
      "\t reading subject           M0371\n",
      "\t reading subject           M0801\n",
      "\t reading subject           M0688\n",
      "\t reading subject           M0160\n",
      "\t reading subject           M0404\n",
      "\t reading subject           M0655\n",
      "\t reading subject           M0062\n",
      "\t reading subject           M0836\n",
      "\t reading subject           M0519\n"
     ]
    }
   ],
   "source": [
    "for class_index, class_id in enumerate(class_set):\n",
    "    print('Working on class {:>5}'.format(class_id))\n",
    "\n",
    "    target_list = read_target_list(class_id,class_sizes[class_index])\n",
    "    for subj_id in target_list:\n",
    "        print('\\t reading subject {:>15}'.format(subj_id))\n",
    "        thickness_wb = get_features(work_dir, subj_id)\n",
    "\n",
    "        # adding the sample to the dataset\n",
    "        dataset.add_sample(subj_id, thickness_wb, class_index, class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice. Isn't it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's nice about this, you say? *The simple fact that you are constructing a dataset as you read the data* in its most elemental form (in the units of the dataset such as the subject ID in our neuroimaging application). You're done as soon as you're done reading the features from disk.\n",
    "\n",
    "What's more - you can inspect the dataset in an intuitive manner, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "45 samples and 4 features.\n",
       "Class Cntrl : 15 samples.\n",
       "Class   MCI : 18 samples.\n",
       "Class Alzmr : 12 samples."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, right? No more too much typing of several commands to get the complete and concise sense of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenient attributes\n",
    "\n",
    "If you would like, you can always get more specific information, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alzmr', 'Cntrl', 'MCI'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Alzmr': 12, 'Cntrl': 15, 'MCI': 18})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes['Cntrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to take a look data inside for few subjects - shall we call it a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0072': array([ 0.72,  0.8 ,  0.25,  0.97]),\n",
       " 'C0097': array([ 0.12,  0.6 ,  0.91,  0.95]),\n",
       " 'C0195': array([ 0.56,  0.1 ,  0.55,  0.35]),\n",
       " 'C0521': array([ 0.95,  0.3 ,  0.55,  0.29]),\n",
       " 'C0846': array([ 0.07,  0.64,  0.51,  0.53])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the number of items to glance, by passing a number to dataset.glance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0097': array([ 0.12,  0.6 ,  0.91,  0.95]),\n",
       " 'C0521': array([ 0.95,  0.3 ,  0.55,  0.29])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you may be wondering what are the subject IDs in the dataset.. here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0521',\n",
       " 'C0097',\n",
       " 'C0195',\n",
       " 'C0846',\n",
       " 'C0072',\n",
       " 'C0074',\n",
       " 'C0299',\n",
       " 'C0232',\n",
       " 'C0847',\n",
       " 'C0427',\n",
       " 'C0380',\n",
       " 'C0888',\n",
       " 'C0078',\n",
       " 'C0164',\n",
       " 'C0662',\n",
       " 'A0048',\n",
       " 'A0060',\n",
       " 'A0914',\n",
       " 'A0304',\n",
       " 'A0339',\n",
       " 'A0363',\n",
       " 'A0132',\n",
       " 'A0645',\n",
       " 'A0547',\n",
       " 'A0943',\n",
       " 'A0266',\n",
       " 'A0207',\n",
       " 'M0804',\n",
       " 'M0084',\n",
       " 'M0945',\n",
       " 'M0518',\n",
       " 'M0940',\n",
       " 'M0365',\n",
       " 'M0222',\n",
       " 'M0441',\n",
       " 'M0725',\n",
       " 'M0371',\n",
       " 'M0801',\n",
       " 'M0688',\n",
       " 'M0160',\n",
       " 'M0404',\n",
       " 'M0655',\n",
       " 'M0062',\n",
       " 'M0836',\n",
       " 'M0519']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iteration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration over samples\n",
    "\n",
    "Thanks to its dictionary based implementation, data for a given sample '007_S_1248' can simply be obtained by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0363 [ 0.84  0.68  0.07  0.46]\n"
     ]
    }
   ],
   "source": [
    "sample_id = dataset.sample_ids[20]\n",
    "print sample_id, dataset.data[sample_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can easily iterate over all the samples to obtain their data as well as class labels. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0521 :      Cntrl : [ 0.95  0.3   0.55  0.29]\n",
      "C0097 :      Cntrl : [ 0.12  0.6   0.91  0.95]\n",
      "C0195 :      Cntrl : [ 0.56  0.1   0.55  0.35]\n",
      "C0846 :      Cntrl : [ 0.07  0.64  0.51  0.53]\n",
      "C0072 :      Cntrl : [ 0.72  0.8   0.25  0.97]\n",
      "C0074 :      Cntrl : [ 0.39  0.03  0.07  0.31]\n",
      "C0299 :      Cntrl : [ 0.13  0.88  0.8   0.1 ]\n",
      "C0232 :      Cntrl : [ 0.02  0.62  0.08  0.35]\n",
      "C0847 :      Cntrl : [ 0.97  0.83  0.71  0.68]\n",
      "C0427 :      Cntrl : [ 0.27  0.57  0.25  0.59]\n",
      "C0380 :      Cntrl : [ 0.42  0.22  0.14  0.14]\n",
      "C0888 :      Cntrl : [ 0.74  0.95  0.27  0.98]\n",
      "C0078 :      Cntrl : [ 0.28  0.69  0.37  0.61]\n",
      "C0164 :      Cntrl : [ 0.4   0.51  0.4   0.11]\n",
      "C0662 :      Cntrl : [ 0.96  0.22  0.04  0.13]\n",
      "A0048 :      Alzmr : [ 0.38  0.42  0.06  0.11]\n",
      "A0060 :      Alzmr : [ 0.22  0.89  0.17  0.29]\n",
      "A0914 :      Alzmr : [ 0.01  0.15  0.43  0.98]\n",
      "A0304 :      Alzmr : [ 0.9   0.22  0.    0.47]\n",
      "A0339 :      Alzmr : [ 0.64  0.5   0.67  0.2 ]\n",
      "A0363 :      Alzmr : [ 0.84  0.68  0.07  0.46]\n",
      "A0132 :      Alzmr : [ 0.98  0.22  0.16  0.92]\n",
      "A0645 :      Alzmr : [ 0.3   0.39  0.33  0.43]\n",
      "A0547 :      Alzmr : [ 0.64  0.98  0.07  0.09]\n",
      "A0943 :      Alzmr : [ 0.42  0.43  0.6   0.37]\n",
      "A0266 :      Alzmr : [ 0.74  0.26  0.42  0.54]\n",
      "A0207 :      Alzmr : [ 0.12  0.59  0.93  0.45]\n",
      "M0804 :        MCI : [ 0.12  0.75  0.07  0.69]\n",
      "M0084 :        MCI : [ 0.88  0.74  0.61  0.89]\n",
      "M0945 :        MCI : [ 0.43  0.74  0.36  0.96]\n",
      "M0518 :        MCI : [ 0.53  0.96  0.63  0.15]\n",
      "M0940 :        MCI : [ 0.51  0.03  0.63  0.26]\n",
      "M0365 :        MCI : [ 0.83  0.52  0.4   0.89]\n",
      "M0222 :        MCI : [ 0.71  0.74  0.15  0.91]\n",
      "M0441 :        MCI : [ 0.35  0.71  0.26  0.82]\n",
      "M0725 :        MCI : [ 0.66  0.16  0.47  0.15]\n",
      "M0371 :        MCI : [ 0.05  0.1   0.94  0.99]\n",
      "M0801 :        MCI : [ 1.    0.41  0.99  0.24]\n",
      "M0688 :        MCI : [ 0.95  0.44  0.22  0.27]\n",
      "M0160 :        MCI : [ 0.81  0.24  0.9   0.95]\n",
      "M0404 :        MCI : [ 0.88  0.85  0.9   0.12]\n",
      "M0655 :        MCI : [ 0.93  0.36  0.3   0.06]\n",
      "M0062 :        MCI : [ 0.88  0.6   0.79  0.25]\n",
      "M0836 :        MCI : [ 0.95  0.6   0.47  0.55]\n",
      "M0519 :        MCI : [ 0.82  0.28  0.68  0.57]\n"
     ]
    }
   ],
   "source": [
    "for sample, features in dataset.data.items():\n",
    "    print \"{} : {:>10} : {}\".format(sample, dataset.classes[sample], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the choice of the OrderedDict() for each of the data, classes and labels, the order of sample addition is retained. Hence the correspondence across samples in the dataset not only key-wise (by the sample id), but also index-wise.\n",
    "\n",
    "Another example to illustrate how one can access the subset of features e.g. cortical thickness for a particular region of interest (say posterior cingulate gyrus) is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a function to return the indices for the ROI\n",
    "def get_ROI_indices(ctx_label=None):\n",
    "    if ctx_label == 'post_cingulate_gyrus':\n",
    "        return xrange(2) # dummy for now\n",
    "    else:\n",
    "        return xrange(dataset.num_features) # all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following code iterates over each sample and prints the average cortical thickness in the specific ROI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0521      Cntrl  0.62\n",
      "C0097      Cntrl  0.36\n",
      "C0195      Cntrl  0.33\n",
      "C0846      Cntrl  0.36\n",
      "C0072      Cntrl  0.76\n",
      "C0074      Cntrl  0.21\n",
      "C0299      Cntrl  0.50\n",
      "C0232      Cntrl  0.32\n",
      "C0847      Cntrl  0.90\n",
      "C0427      Cntrl  0.42\n",
      "C0380      Cntrl  0.32\n",
      "C0888      Cntrl  0.84\n",
      "C0078      Cntrl  0.48\n",
      "C0164      Cntrl  0.46\n",
      "C0662      Cntrl  0.59\n",
      "A0048      Alzmr  0.40\n",
      "A0060      Alzmr  0.56\n",
      "A0914      Alzmr  0.08\n",
      "A0304      Alzmr  0.56\n",
      "A0339      Alzmr  0.57\n",
      "A0363      Alzmr  0.76\n",
      "A0132      Alzmr  0.60\n",
      "A0645      Alzmr  0.34\n",
      "A0547      Alzmr  0.81\n",
      "A0943      Alzmr  0.43\n",
      "A0266      Alzmr  0.50\n",
      "A0207      Alzmr  0.36\n",
      "M0804        MCI  0.43\n",
      "M0084        MCI  0.81\n",
      "M0945        MCI  0.59\n",
      "M0518        MCI  0.74\n",
      "M0940        MCI  0.27\n",
      "M0365        MCI  0.68\n",
      "M0222        MCI  0.73\n",
      "M0441        MCI  0.53\n",
      "M0725        MCI  0.41\n",
      "M0371        MCI  0.08\n",
      "M0801        MCI  0.70\n",
      "M0688        MCI  0.70\n",
      "M0160        MCI  0.53\n",
      "M0404        MCI  0.86\n",
      "M0655        MCI  0.65\n",
      "M0062        MCI  0.74\n",
      "M0836        MCI  0.77\n",
      "M0519        MCI  0.55\n"
     ]
    }
   ],
   "source": [
    "avg_thickness = dict()\n",
    "for sample, features in dataset.data.items():\n",
    "    avg_thickness[sample] = np.mean(features[get_ROI_indices('post_cingulate_gyrus')])\n",
    "    print \"{} {:>10}  {:.2f}\".format(sample, dataset.classes[sample], avg_thickness[sample] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bar plot with the just computed numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20828357773861217,\n",
       " 0.75855481225358801,\n",
       " 0.35969326688292214,\n",
       " 0.6759115652357176,\n",
       " 0.4818775555718694,\n",
       " 0.39634091997594362,\n",
       " 0.86307529531573035,\n",
       " 0.55668829695017452,\n",
       " 0.45693408819469711,\n",
       " 0.55752594793326526,\n",
       " 0.74104242282296784,\n",
       " 0.54989676544637989,\n",
       " 0.70492586478718477,\n",
       " 0.34250331698673958,\n",
       " 0.43084823279734125,\n",
       " 0.31924955446818032,\n",
       " 0.76108599112941877,\n",
       " 0.58520892910233835,\n",
       " 0.2714500820505506,\n",
       " 0.42729278990200026,\n",
       " 0.41955539057127744,\n",
       " 0.53414583423011819,\n",
       " 0.082471414289991885,\n",
       " 0.52840772865515762,\n",
       " 0.80740043893044477,\n",
       " 0.73725807678203437,\n",
       " 0.075305533117054568,\n",
       " 0.62329444259653555,\n",
       " 0.58902769011073119,\n",
       " 0.329880617604145,\n",
       " 0.59830751815668781,\n",
       " 0.49809014419921327,\n",
       " 0.57075014953794412,\n",
       " 0.41091825737010768,\n",
       " 0.69586104344867072,\n",
       " 0.32196134103385143,\n",
       " 0.35529354358654569,\n",
       " 0.81051757625010179,\n",
       " 0.50303578284141759,\n",
       " 0.72748460530019976,\n",
       " 0.64663513739419853,\n",
       " 0.35701920701003542,\n",
       " 0.77397417189390749,\n",
       " 0.84499071643523038,\n",
       " 0.90151247091865372]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_thickness.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f745f1126d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(avg_thickness.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember as the original source of data was random, this has no units, property or meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subsetselection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset selection\n",
    "\n",
    "In addition to the structured way of obtaining the various properties of this dataset, this implementation really will come in handy when you have to slice and dice the dataset (with large number of classes and features) into smaller subsets (e.g. for binary classification). Let's see how we can retrieve the data for a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctrl = dataset.get_class('Cntrl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, obtaining the data for a given class is a simple call away.\n",
    "\n",
    "Now let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "15 samples and 4 features.\n",
       "Class Cntrl : 15 samples."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with updated description automatically, to indicate its history. Let's see some data from controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0097': array([ 0.12,  0.6 ,  0.91,  0.95]),\n",
       " 'C0521': array([ 0.95,  0.3 ,  0.55,  0.29])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query a random subset of samples for manual inspection or cross-validation purposes. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "14 samples and 4 features.\n",
       "Class Cntrl : 5 samples.\n",
       "Class   MCI : 5 samples.\n",
       "Class Alzmr : 4 samples."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_subset = dataset.random_subset(perc_per_class=0.3)\n",
    "random_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see which samples were selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0097',\n",
       " 'C0299',\n",
       " 'C0427',\n",
       " 'C0380',\n",
       " 'C0078',\n",
       " 'A0048',\n",
       " 'A0645',\n",
       " 'A0943',\n",
       " 'A0266',\n",
       " 'M0804',\n",
       " 'M0518',\n",
       " 'M0441',\n",
       " 'M0725',\n",
       " 'M0371']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_subset.sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that it is indeed random by issuing another call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0521',\n",
       " 'C0097',\n",
       " 'C0299',\n",
       " 'C0847',\n",
       " 'C0888',\n",
       " 'A0339',\n",
       " 'A0645',\n",
       " 'A0547',\n",
       " 'A0266',\n",
       " 'M0804',\n",
       " 'M0945',\n",
       " 'M0518',\n",
       " 'M0222',\n",
       " 'M0062']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supplying a new seed everytime to ensure randomization\n",
    "from datetime import datetime\n",
    "dataset.random_subset(perc_per_class=0.3, random_seed=datetime.now()).sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how we can retrieve specific samples by their IDs (for which there are many use cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "19 samples and 4 features.\n",
       "Class Cntrl : 14 samples.\n",
       "Class Alzmr : 5 samples."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.get_subset(dataset.sample_ids[1:20])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More useful case would be to select a subset of classes from an original large dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "45 samples and 4 features.\n",
       "Class Cntrl : 15 samples.\n",
       "Class   MCI : 18 samples.\n",
       "Class Alzmr : 12 samples."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "27 samples and 4 features.\n",
       "Class Cntrl : 15 samples.\n",
       "Class Alzmr : 12 samples."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset = dataset.get_class(['Cntrl','Alzmr'])\n",
    "binary_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about selecting a subset of features from all samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subset features derived from: \n",
       " \n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "27 samples and 2 features.\n",
       "Class Cntrl : 15 samples.\n",
       "Class Alzmr : 12 samples."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset.get_feature_subset(xrange(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great.** Isn't it? You can also see the two-time-point history (initial subset in classes, followed by a subset in features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='serialization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Once you have this dataset, you can save and load these trivially using your favourite serialization module. Let's do some pickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno Unable to read the dataset from file: {}] [Errno 2] No such file or directory: '/project/ADNI/FreesurferThickness_v4p3/binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-4f44d193e0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbinary_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/praamana/repos/pyradigm/pyradigm.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             df)\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mioe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to read the dataset from file: {}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mioe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno Unable to read the dataset from file: {}] [Errno 2] No such file or directory: '/project/ADNI/FreesurferThickness_v4p3/binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl'"
     ]
    }
   ],
   "source": [
    "out_file = os.path.join(work_dir,'binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl')\n",
    "binary_dataset.save(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's it - it is saved.\n",
    "\n",
    "Let's reload it from disk and make sure we can indeed retrieve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reloaded = MLDataset(filepath=out_file) # another form of the constructor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arithmetic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Arithmetic\n",
    "\n",
    "You might wonder how can you combine two different types of features ( thickness and shape ) from the dataset. Piece of cake, see below ...\n",
    "\n",
    "To concatenat two datasets, first we make a second dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_two = MLDataset(in_dataset=dataset) # yet another constructor: in its copy form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you check if they are \"functionally identical\"? As in same keys, same data and classes for each key... Easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_two == dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the arithmentic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = dataset + dataset_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. The add method recognized the identical set of keys and performed a horiz cat, as can be noticed by the twice the number of features in the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some removal in similar fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smaller = combined - dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure is even producing a warning to let you know the resulting output would be empty! We can verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bool(smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='portability'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good. How does it interact with other packages out there, you might ask? It is as simple as you can imagine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(binary_dataset.data_matrix, binary_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, a simple example to show you the utility and convenience of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for checking it out. \n",
    "\n",
    "### I would appreciate if you could give me feedback on improving or sharpening it further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
