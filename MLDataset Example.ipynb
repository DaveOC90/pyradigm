{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python class to encapsulate a machine learning dataset based on dictionares. \n",
    "\n",
    "This class is greatly suited for neuroimaging applications (or any other domain), where each sample needs to be uniquely identified with a subject ID (or something similar). \n",
    "\n",
    "Key-level correspondence across data, labels (1 or 2), classnames ('healthy', 'disease') and the related helps maintain data integrity, in addition to enabling traceback to original sources from where the features have been originally derived.\n",
    "\n",
    "An example application is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improting the class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mldataset import MLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to import it and give it a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = MLDataset()\n",
    "dataset.description = 'ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the dataset some description attached to it, however we know it is empty. This can be verified in a boolean context as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's add samples to this dataset which is when this dataset implementation becomes really handy. Before we do that, we will improt some convenience routines defined to just illustrate a simple yet common use of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_thickness(path):\n",
    "    \"\"\"Dummy function to minic a data reader.\"\"\"\n",
    "\n",
    "    # in your actural routine, this might be:\n",
    "    #   pysurfer.read_thickness(path).values()\n",
    "    return np.random.random(8)\n",
    "\n",
    "\n",
    "def get_features(work_dir, subj_id):\n",
    "    \"\"\"Returns the whole brain cortical thickness for a given subject ID.\"\"\"\n",
    "\n",
    "    # extension to identify the data file; this could be .curv, anything else you choose\n",
    "    ext_thickness = '.thickness'\n",
    "\n",
    "    thickness = dict()\n",
    "    for hemi in ['lh', 'rh']:\n",
    "        path_thickness = os.path.join(work_dir, subj_id, hemi + ext_thickness)\n",
    "        thickness[hemi] = read_thickness(path_thickness)\n",
    "\n",
    "    # concatenating them to build a whole brain feature set\n",
    "    thickness_wb = np.concatenate([thickness['lh'], thickness['rh']])\n",
    "\n",
    "    return thickness_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a IO functions to read the data for us. Let's define where the data will come from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_dir = '/project/ADNI/FreesurferThickness_v4p3'\n",
    "class_set = ['Ctrl', 'Alzr', 'Another']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would obviously change for you applications, but this has sufficient properties to illustrate the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what methods this dataset offers us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_classes',\n",
       " 'add_sample',\n",
       " 'class_set',\n",
       " 'class_sizes',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'data_matrix',\n",
       " 'description',\n",
       " 'get_class',\n",
       " 'get_subset',\n",
       " 'keys',\n",
       " 'labels',\n",
       " 'num_classes',\n",
       " 'num_features',\n",
       " 'num_samples',\n",
       " 'subject_ids']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there few methods such as add_sample, get_subset etc: important method being add_sample, which is key to constructing this dataset. Let's go ahead and some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on class  Ctrl\n",
      "\t reading subject      011_S_0005\n",
      "\t reading subject      011_S_0008\n",
      "\t reading subject      022_S_0014\n",
      "\t reading subject      100_S_0015\n",
      "\t reading subject      011_S_0016\n",
      "\t reading subject      067_S_0019\n",
      "\t reading subject      011_S_0021\n",
      "\t reading subject      011_S_0022\n",
      "\t reading subject      011_S_0023\n",
      "\t reading subject      023_S_0031\n",
      "Working on class  Alzr\n",
      "\t reading subject      031_S_1209\n",
      "\t reading subject      007_S_1248\n",
      "\t reading subject      007_S_1304\n",
      "\t reading subject      009_S_1334\n",
      "\t reading subject      007_S_1339\n",
      "\t reading subject      005_S_1341\n",
      "\t reading subject      057_S_1371\n",
      "\t reading subject      057_S_1379\n",
      "\t reading subject      041_S_1391\n",
      "\t reading subject      094_S_1402\n",
      "Working on class Another\n",
      "\t reading subject      130_S_1200\n",
      "\t reading subject      130_S_1201\n",
      "\t reading subject      130_S_1290\n",
      "\t reading subject      130_S_1337\n",
      "\t reading subject      131_S_0123\n",
      "\t reading subject      131_S_0319\n",
      "\t reading subject      131_S_0384\n",
      "\t reading subject      131_S_0409\n",
      "\t reading subject      131_S_0436\n",
      "\t reading subject      131_S_0441\n"
     ]
    }
   ],
   "source": [
    "for class_index, class_id in enumerate(class_set):\n",
    "    print('Working on class {:>5}'.format(class_id))\n",
    "\n",
    "    target_list_path = os.path.join(work_dir,'scripts','test_sample.{}'.format(class_id))\n",
    "    with open(target_list_path,'r') as tf:\n",
    "        target_list = tf.readlines()\n",
    "        target_list = [sub.strip() for sub in target_list]\n",
    "\n",
    "    for subj_id in target_list:\n",
    "        print('\\t reading subject {:>15}'.format(subj_id))\n",
    "        thickness_wb = get_features(work_dir, subj_id)\n",
    "\n",
    "        # adding the sample to the dataset\n",
    "        dataset.add_sample(subj_id, thickness_wb, class_index, class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice. Isn't it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's nice about this, you say? *The simple fact that you are constructing a dataset as you read the data* in its most elemental form (in the units of the dataset such as the subject ID in our neuroimaging application). You're done as soon as you're done reading the features from disk.\n",
    "\n",
    "What's more - you can inspect the dataset in an intuitive manner, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "30 samples and 16 features.\n",
       "Class    Alzr : 10 samples.\n",
       "Class Another : 10 samples.\n",
       "Class    Ctrl : 10 samples."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, right? No more too much typing of several commands to get the complete and concise sense of the dataset.\n",
    "\n",
    "If you would like, you can always get more specific information, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alzr', 'Another', 'Ctrl'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Alzr': 10, 'Another': 10, 'Ctrl': 10})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes['Ctrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['011_S_0005',\n",
       " '011_S_0008',\n",
       " '022_S_0014',\n",
       " '100_S_0015',\n",
       " '011_S_0016',\n",
       " '067_S_0019',\n",
       " '011_S_0021',\n",
       " '011_S_0022',\n",
       " '011_S_0023',\n",
       " '023_S_0031',\n",
       " '031_S_1209',\n",
       " '007_S_1248',\n",
       " '007_S_1304',\n",
       " '009_S_1334',\n",
       " '007_S_1339',\n",
       " '005_S_1341',\n",
       " '057_S_1371',\n",
       " '057_S_1379',\n",
       " '041_S_1391',\n",
       " '094_S_1402',\n",
       " '130_S_1200',\n",
       " '130_S_1201',\n",
       " '130_S_1290',\n",
       " '130_S_1337',\n",
       " '131_S_0123',\n",
       " '131_S_0319',\n",
       " '131_S_0384',\n",
       " '131_S_0409',\n",
       " '131_S_0436',\n",
       " '131_S_0441']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.subject_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the structured way of obtaining the various properties of this dataset, this implementation really will come in handy when you have to slice and dice the dataset (with large number of classes and features) into smaller subsets (e.g. for binary classification). Let's see how we can retrieve the data for a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctrl = dataset.get_class('Ctrl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, a simple call way.\n",
    "\n",
    "Now let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "10 samples and 16 features.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with updated description automatically, to indicate its history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how we can retrieve specific subject IDs (for which there are many use cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset.get_subset(['022_S_0014','023_S_0031','023_S_0031','131_S_0409'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as simple as that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "3 samples and 16 features.\n",
       "Class Another : 1 samples.\n",
       "Class    Ctrl : 2 samples."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More useful case would be to select a subset of classes from an original large dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_dataset = dataset.get_class(['Ctrl','Alzr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "20 samples and 16 features.\n",
       "Class Alzr : 10 samples.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great.**\n",
    "\n",
    "Once you have this dataset, you can save and load these trivially using your favourite serialization module. Let's do some pickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "out_file = os.path.join(work_dir,'binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl')\n",
    "\n",
    "# saving the dataset to disk\n",
    "try:\n",
    "    path = os.path.abspath(out_file)\n",
    "    with open(path, 'wb') as df:\n",
    "        pickle.dump(dataset, df)\n",
    "    print('saved.')\n",
    "except:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, a simple example to show you the utility and convenience of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for checking it out. I would appreciate if you could give me feedback on improving or sharpening it further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
