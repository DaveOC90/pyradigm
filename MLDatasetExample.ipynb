{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "* <a href='#motivation'>Motivation</a>\n",
    "\n",
    "* <a href='#constructor'>Constructor</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Serialization</a>\n",
    "\n",
    "* <a href='#arithmetic'>Arithmetic</a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python data structure to improve handling of datasets in machine learning workflows\n",
    "\n",
    "This class is greatly suited for neuroimaging applications (or any other domain), where each sample needs to be uniquely identified with a subject ID (or something similar). \n",
    "\n",
    "Key-level correspondence across data, labels (1 or 2), classnames ('healthy', 'disease') and the related helps maintain data integrity and improve the provenance, in addition to enabling traceback to original sources from where the features have been originally derived.\n",
    "\n",
    "Just to given you a concrete examples, let's look at how an ML dataset is handled traditionally.\n",
    "\n",
    "You have a matrix X of size n x p, with n samples and p features, and a vector y containing the target values (or class labels or class identifiers). This X and y serves as training (and test set) for a classifier like SVM to fit the data X to match y as accurately as possible.\n",
    "\n",
    "Let's get a little more concrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : \n",
      "[[ 0.93  0.13  0.18]\n",
      " [ 0.02  0.39  0.4 ]\n",
      " [ 0.35  0.63  0.51]\n",
      " [ 0.26  0.77  0.4 ]\n",
      " [ 0.1   0.67  0.7 ]\n",
      " [ 0.77  0.44  0.47]\n",
      " [ 0.46  0.17  0.15]\n",
      " [ 0.58  0.97  0.48]\n",
      " [ 0.43  0.39  0.79]\n",
      " [ 0.15  0.82  0.7 ]]\n",
      "y : \n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "n = 10 # number of samples\n",
    "p = 3  # number of features\n",
    "\n",
    "X = np.random.random([n, p]) # random data for illustration\n",
    "y = [1]*5 + [2]*5            # random labels ...\n",
    "\n",
    "np.set_printoptions(precision=2) # save some screen space\n",
    "print 'X : \\n', X\n",
    "print 'y : \\n', y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the machine learning toolboxes take their input in this form: X and y, regardless of the original source that produced these features in the first place.\n",
    "\n",
    "This is all fine if all you ever wanted to do is to extract some features, do some machine learning and dispose these features away! \n",
    "\n",
    "** But this is almost never the case!**\n",
    "\n",
    "Because it doesn't simply end there.\n",
    "\n",
    "At a minimum, I often need to know \n",
    " * which samples are misclassified - meaning you need to know what the identifiers are and not simply their row indices in X?\n",
    " * what are the charecteristics of those samples?\n",
    " * what classes do they belong to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all this info needs to be obtained\n",
    " * without having to write lots of code connecting few non-obvious links to disparate sources of data (numerical features X, and sample identifiers in a CSV file) to find the relevant info\n",
    " * without having to track down who or which method originally produced these features\n",
    " * how the previous personnel or grad student organized the whole dataset, if you haven't generated the features yourself from scratch\n",
    "\n",
    "And if you are like me, you would be thinking about how would you organize your workflow such that the aforementioned tasks can be accomplished with ease.\n",
    " \n",
    "This data structure attempts to accomplish that with ease. By always organizing the extracted features keyed-in into a dictionary with their *sample id*, and other important info such as *target values* and other identified info. This, by definition, preserves the integrity of the data (inability to incorrectly label samples etc).\n",
    "\n",
    "No, this data structure doesn't offer the full [provenance tracking](http://rrcns.readthedocs.io/en/latest/provenance_tracking.html), which is quite a challenging problem. But it tries make your life a little easier in your ML workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example application is shown below, touching upon the following topics:\n",
    "\n",
    "* <a href='#constructor'>Constructor</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Serialization</a>\n",
    "\n",
    "* <a href='#arithmetic'>Arithmetic</a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improting the necessary modules and our fancy class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mldataset import MLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate it and give it a description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = MLDataset()\n",
    "dataset.description = 'ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "Empty dataset."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the dataset some description attached to it, however we know it is empty. This can be verified in a boolean context as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's add samples to this dataset which is when this dataset implementation becomes really handy. Before we do that, we will define some convenience routines defined to just illustrate a simple yet common use of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_thickness(path):\n",
    "    \"\"\"Dummy function to minic a data reader.\"\"\"\n",
    "\n",
    "    # in your actural routine, this might be:\n",
    "    #   pysurfer.read_thickness(path).values()\n",
    "    return np.random.random(2)\n",
    "\n",
    "\n",
    "def get_features(work_dir, subj_id):\n",
    "    \"\"\"Returns the whole brain cortical thickness for a given subject ID.\"\"\"\n",
    "\n",
    "    # extension to identify the data file; this could be .curv, anything else you choose\n",
    "    ext_thickness = '.thickness'\n",
    "\n",
    "    thickness = dict()\n",
    "    for hemi in ['lh', 'rh']:\n",
    "        path_thickness = os.path.join(work_dir, subj_id, hemi + ext_thickness)\n",
    "        thickness[hemi] = read_thickness(path_thickness)\n",
    "\n",
    "    # concatenating them to build a whole brain feature set\n",
    "    thickness_wb = np.concatenate([thickness['lh'], thickness['rh']])\n",
    "\n",
    "    return thickness_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have IO routines to read the data for us. Let's define where the data will come from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_dir = '/project/ADNI/FreesurferThickness_v4p3'\n",
    "class_set = ['Ctrl', 'Alzr', 'Another']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would obviously change for your applications, but this has sufficient properties to illustrate the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what methods this dataset offers us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_classes',\n",
       " 'add_sample',\n",
       " 'class_set',\n",
       " 'class_sizes',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'data_matrix',\n",
       " 'del_sample',\n",
       " 'description',\n",
       " 'extend',\n",
       " 'get_class',\n",
       " 'get_feature_subset',\n",
       " 'get_subset',\n",
       " 'glance',\n",
       " 'keys',\n",
       " 'num_classes',\n",
       " 'num_features',\n",
       " 'num_samples',\n",
       " 'sample_ids',\n",
       " 'save',\n",
       " 'target']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='constructor'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor\n",
    "\n",
    "You can see there few methods such as add_sample, get_subset etc: important method being add_sample, which is key to constructing this dataset. Let's go ahead and some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on class  Ctrl\n",
      "\t reading subject      011_S_0005\n",
      "\t reading subject      011_S_0008\n",
      "\t reading subject      022_S_0014\n",
      "\t reading subject      100_S_0015\n",
      "\t reading subject      011_S_0016\n",
      "\t reading subject      067_S_0019\n",
      "\t reading subject      011_S_0021\n",
      "\t reading subject      011_S_0022\n",
      "\t reading subject      011_S_0023\n",
      "\t reading subject      023_S_0031\n",
      "Working on class  Alzr\n",
      "\t reading subject      031_S_1209\n",
      "\t reading subject      007_S_1248\n",
      "\t reading subject      007_S_1304\n",
      "\t reading subject      009_S_1334\n",
      "\t reading subject      007_S_1339\n",
      "\t reading subject      005_S_1341\n",
      "\t reading subject      057_S_1371\n",
      "\t reading subject      057_S_1379\n",
      "\t reading subject      041_S_1391\n",
      "\t reading subject      094_S_1402\n",
      "Working on class Another\n",
      "\t reading subject      130_S_1200\n",
      "\t reading subject      130_S_1201\n",
      "\t reading subject      130_S_1290\n",
      "\t reading subject      130_S_1337\n",
      "\t reading subject      131_S_0123\n",
      "\t reading subject      131_S_0319\n",
      "\t reading subject      131_S_0384\n",
      "\t reading subject      131_S_0409\n",
      "\t reading subject      131_S_0436\n",
      "\t reading subject      131_S_0441\n"
     ]
    }
   ],
   "source": [
    "for class_index, class_id in enumerate(class_set):\n",
    "    print('Working on class {:>5}'.format(class_id))\n",
    "\n",
    "    target_list_path = os.path.join(work_dir,'scripts','test_sample.{}'.format(class_id))\n",
    "    with open(target_list_path,'r') as tf:\n",
    "        target_list = tf.readlines()\n",
    "        target_list = [sub.strip() for sub in target_list]\n",
    "\n",
    "    for subj_id in target_list:\n",
    "        print('\\t reading subject {:>15}'.format(subj_id))\n",
    "        thickness_wb = get_features(work_dir, subj_id)\n",
    "\n",
    "        # adding the sample to the dataset\n",
    "        dataset.add_sample(subj_id, thickness_wb, class_index, class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice. Isn't it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's nice about this, you say? *The simple fact that you are constructing a dataset as you read the data* in its most elemental form (in the units of the dataset such as the subject ID in our neuroimaging application). You're done as soon as you're done reading the features from disk.\n",
    "\n",
    "What's more - you can inspect the dataset in an intuitive manner, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "30 samples and 4 features.\n",
       "Class    Alzr : 10 samples.\n",
       "Class Another : 10 samples.\n",
       "Class    Ctrl : 10 samples."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, right? No more too much typing of several commands to get the complete and concise sense of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenient attributes\n",
    "\n",
    "If you would like, you can always get more specific information, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alzr', 'Another', 'Ctrl'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Alzr': 10, 'Another': 10, 'Ctrl': 10})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_sizes['Ctrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to take a look data inside for few subjects - shall we call it a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'011_S_0005': array([ 0.23,  0.16,  0.93,  0.41]),\n",
       " '011_S_0008': array([ 0.52,  0.51,  0.09,  0.73]),\n",
       " '011_S_0016': array([ 0.14,  0.23,  0.53,  0.03]),\n",
       " '022_S_0014': array([ 0.82,  0.31,  0.77,  0.93]),\n",
       " '100_S_0015': array([ 0.33,  0.34,  0.84,  0.98])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the number of items to glance, by passing a number to dataset.glance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'011_S_0005': array([ 0.23,  0.16,  0.93,  0.41]),\n",
       " '011_S_0008': array([ 0.52,  0.51,  0.09,  0.73])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you may be wondering what are the subject IDs in the dataset.. here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['011_S_0005',\n",
       " '011_S_0008',\n",
       " '022_S_0014',\n",
       " '100_S_0015',\n",
       " '011_S_0016',\n",
       " '067_S_0019',\n",
       " '011_S_0021',\n",
       " '011_S_0022',\n",
       " '011_S_0023',\n",
       " '023_S_0031',\n",
       " '031_S_1209',\n",
       " '007_S_1248',\n",
       " '007_S_1304',\n",
       " '009_S_1334',\n",
       " '007_S_1339',\n",
       " '005_S_1341',\n",
       " '057_S_1371',\n",
       " '057_S_1379',\n",
       " '041_S_1391',\n",
       " '094_S_1402',\n",
       " '130_S_1200',\n",
       " '130_S_1201',\n",
       " '130_S_1290',\n",
       " '130_S_1337',\n",
       " '131_S_0123',\n",
       " '131_S_0319',\n",
       " '131_S_0384',\n",
       " '131_S_0409',\n",
       " '131_S_0436',\n",
       " '131_S_0441']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iteration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration over samples\n",
    "\n",
    "Thanks to its dictionary based implementation, data for a given sample '007_S_1248' can simply be obtained by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67,  0.67,  0.94,  0.22])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['007_S_1248']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can easily iterate over all the samples to obtain their data as well as class labels. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011_S_0005 :       Ctrl : [ 0.23  0.16  0.93  0.41]\n",
      "011_S_0008 :       Ctrl : [ 0.52  0.51  0.09  0.73]\n",
      "022_S_0014 :       Ctrl : [ 0.82  0.31  0.77  0.93]\n",
      "100_S_0015 :       Ctrl : [ 0.33  0.34  0.84  0.98]\n",
      "011_S_0016 :       Ctrl : [ 0.14  0.23  0.53  0.03]\n",
      "067_S_0019 :       Ctrl : [ 0.91  0.25  0.31  0.88]\n",
      "011_S_0021 :       Ctrl : [ 0.6   0.28  0.63  0.15]\n",
      "011_S_0022 :       Ctrl : [ 0.94  0.85  0.11  0.25]\n",
      "011_S_0023 :       Ctrl : [  2.53e-01   1.11e-01   3.92e-01   2.11e-04]\n",
      "023_S_0031 :       Ctrl : [ 0.78  0.77  0.11  0.98]\n",
      "031_S_1209 :       Alzr : [ 0.17  0.7   0.91  0.21]\n",
      "007_S_1248 :       Alzr : [ 0.67  0.67  0.94  0.22]\n",
      "007_S_1304 :       Alzr : [ 0.63  0.37  0.51  0.53]\n",
      "009_S_1334 :       Alzr : [ 0.35  0.04  0.44  0.25]\n",
      "007_S_1339 :       Alzr : [ 0.23  0.01  0.57  0.24]\n",
      "005_S_1341 :       Alzr : [ 0.96  0.28  0.97  0.29]\n",
      "057_S_1371 :       Alzr : [ 0.77  0.62  0.11  0.49]\n",
      "057_S_1379 :       Alzr : [ 0.26  0.23  0.99  0.74]\n",
      "041_S_1391 :       Alzr : [ 0.73  0.76  0.38  0.15]\n",
      "094_S_1402 :       Alzr : [ 0.76  0.24  0.28  0.45]\n",
      "130_S_1200 :    Another : [ 0.9   0.41  0.93  0.04]\n",
      "130_S_1201 :    Another : [ 0.8   0.99  0.61  0.89]\n",
      "130_S_1290 :    Another : [ 0.67  0.91  0.05  0.03]\n",
      "130_S_1337 :    Another : [ 0.22  0.63  0.41  0.01]\n",
      "131_S_0123 :    Another : [ 0.61  0.99  0.67  0.45]\n",
      "131_S_0319 :    Another : [ 0.71  0.71  0.52  0.64]\n",
      "131_S_0384 :    Another : [ 0.09  0.26  0.    0.26]\n",
      "131_S_0409 :    Another : [ 0.93  0.05  0.65  0.07]\n",
      "131_S_0436 :    Another : [ 0.12  0.96  0.4   0.68]\n",
      "131_S_0441 :    Another : [ 0.15  0.53  0.57  0.94]\n"
     ]
    }
   ],
   "source": [
    "for sample, features in dataset.data.items():\n",
    "    print \"{} : {:>10} : {}\".format(sample, dataset.classes[sample], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the choice of the OrderedDict() for each of the data, classes and labels, the order of sample addition is retained. Hence the correspondence across samples in the dataset not only key-wise (by the sample id), but also index-wise.\n",
    "\n",
    "Another example to illustrate how one can access the subset of features e.g. cortical thickness for a particular region of interest (say posterior cingulate gyrus) is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a function to return the indices for the ROI\n",
    "def get_ROI_indices(ctx_label=None):\n",
    "    if ctx_label == 'post_cingulate_gyrus':\n",
    "        return xrange(2) # dummy for now\n",
    "    else:\n",
    "        return xrange(dataset.num_features) # all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following code iterates over each sample and prints the average cortical thickness in the specific ROI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011_S_0005       Ctrl  0.19\n",
      "011_S_0008       Ctrl  0.52\n",
      "022_S_0014       Ctrl  0.56\n",
      "100_S_0015       Ctrl  0.33\n",
      "011_S_0016       Ctrl  0.19\n",
      "067_S_0019       Ctrl  0.58\n",
      "011_S_0021       Ctrl  0.44\n",
      "011_S_0022       Ctrl  0.89\n",
      "011_S_0023       Ctrl  0.18\n",
      "023_S_0031       Ctrl  0.78\n",
      "031_S_1209       Alzr  0.43\n",
      "007_S_1248       Alzr  0.67\n",
      "007_S_1304       Alzr  0.50\n",
      "009_S_1334       Alzr  0.20\n",
      "007_S_1339       Alzr  0.12\n",
      "005_S_1341       Alzr  0.62\n",
      "057_S_1371       Alzr  0.70\n",
      "057_S_1379       Alzr  0.24\n",
      "041_S_1391       Alzr  0.75\n",
      "094_S_1402       Alzr  0.50\n",
      "130_S_1200    Another  0.65\n",
      "130_S_1201    Another  0.89\n",
      "130_S_1290    Another  0.79\n",
      "130_S_1337    Another  0.42\n",
      "131_S_0123    Another  0.80\n",
      "131_S_0319    Another  0.71\n",
      "131_S_0384    Another  0.18\n",
      "131_S_0409    Another  0.49\n",
      "131_S_0436    Another  0.54\n",
      "131_S_0441    Another  0.34\n"
     ]
    }
   ],
   "source": [
    "avg_thickness = dict()\n",
    "for sample, features in dataset.data.items():\n",
    "    avg_thickness[sample] = np.mean(features[get_ROI_indices('post_cingulate_gyrus')])\n",
    "    print \"{} {:>10}  {:.2f}\".format(sample, dataset.classes[sample], avg_thickness[sample] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bar plot with the just computed numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhChvAEiI8gaAhP4fNl16lq4OqxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11225cf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "n, bins, patches = plt.hist(avg_thickness.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember as the original source of data was random, this has no units, property or meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subsetselection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset selection\n",
    "\n",
    "In addition to the structured way of obtaining the various properties of this dataset, this implementation really will come in handy when you have to slice and dice the dataset (with large number of classes and features) into smaller subsets (e.g. for binary classification). Let's see how we can retrieve the data for a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctrl = dataset.get_class('Ctrl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, obtaining the data for a given class is a simple call away.\n",
    "\n",
    "Now let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "10 samples and 4 features.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with updated description automatically, to indicate its history. Let's see some data from controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'011_S_0005': array([ 0.23,  0.16,  0.93,  0.41]),\n",
       " '011_S_0008': array([ 0.52,  0.51,  0.09,  0.73])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how we can retrieve specific samples by their IDs (for which there are many use cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset.get_subset(['022_S_0014','023_S_0031','023_S_0031','131_S_0409'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as simple as that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "3 samples and 4 features.\n",
       "Class Another : 1 samples.\n",
       "Class    Ctrl : 2 samples."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More useful case would be to select a subset of classes from an original large dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_dataset = dataset.get_class(['Ctrl','Alzr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "20 samples and 4 features.\n",
       "Class Alzr : 10 samples.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about selecting a subset of features from all samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subset features derived from: \n",
       " \n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "20 samples and 2 features.\n",
       "Class Alzr : 10 samples.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset.get_feature_subset(xrange(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great.** Isn't it? You can also see the two-time-point history (initial subset in classes, followed by a subset in features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='serialization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Once you have this dataset, you can save and load these trivially using your favourite serialization module. Let's do some pickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_file = os.path.join(work_dir,'binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl')\n",
    "binary_dataset.save(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's it - it is saved.\n",
    "\n",
    "Let's reload it from disk and make sure we can indeed retrieve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset from: /project/ADNI/FreesurferThickness_v4p3/binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.pkl\n"
     ]
    }
   ],
   "source": [
    "reloaded = MLDataset(filepath=out_file) # another form of the constructor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
       "20 samples and 4 features.\n",
       "Class Alzr : 10 samples.\n",
       "Class Ctrl : 10 samples."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arithmetic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Arithmetic\n",
    "\n",
    "You might wonder how can you combine two different types of features ( thickness and shape ) from the dataset. Piece of cake, see below ...\n",
    "\n",
    "To concatenat two datasets, first we make a second dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_two = MLDataset(in_dataset=dataset) # yet another constructor: in its copy form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you check if they are \"functionally identical\"? As in same keys, same data and classes for each key... Easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_two == dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the arithmentic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical keys found. Trying to horizontally concatenate features for each sample.\n"
     ]
    }
   ],
   "source": [
    "combined = dataset + dataset_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. The add method recognized the identical set of keys and performed a horiz cat, as can be noticed by the twice the number of features in the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "30 samples and 8 features.\n",
       "Class    Alzr : 10 samples.\n",
       "Class Another : 10 samples.\n",
       "Class    Ctrl : 10 samples."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some removal in similar fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011_S_0005 removed.\n",
      "011_S_0008 removed.\n",
      "022_S_0014 removed.\n",
      "100_S_0015 removed.\n",
      "011_S_0016 removed.\n",
      "067_S_0019 removed.\n",
      "011_S_0021 removed.\n",
      "011_S_0022 removed.\n",
      "011_S_0023 removed.\n",
      "023_S_0031 removed.\n",
      "031_S_1209 removed.\n",
      "007_S_1248 removed.\n",
      "007_S_1304 removed.\n",
      "009_S_1334 removed.\n",
      "007_S_1339 removed.\n",
      "005_S_1341 removed.\n",
      "057_S_1371 removed.\n",
      "057_S_1379 removed.\n",
      "041_S_1391 removed.\n",
      "094_S_1402 removed.\n",
      "130_S_1200 removed.\n",
      "130_S_1201 removed.\n",
      "130_S_1290 removed.\n",
      "130_S_1337 removed.\n",
      "131_S_0123 removed.\n",
      "131_S_0319 removed.\n",
      "131_S_0384 removed.\n",
      "131_S_0409 removed.\n",
      "131_S_0436 removed.\n",
      "131_S_0441 removed.\n"
     ]
    }
   ],
   "source": [
    "smaller = combined - dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure is even producing a warning to let you know the resulting output would be empty! We can verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='portability'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good. How does it interact with other packages out there, you might ask? It is as simple as you can imagine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(binary_dataset.data_matrix, binary_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, a simple example to show you the utility and convenience of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for checking it out. \n",
    "\n",
    "### I would appreciate if you could give me feedback on improving or sharpening it further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
